<h1>Topics covered by me with respect to the dates.</h1>
3 June 24 : It was our Day 1 of Internship. We were given briefing about the flow of internship, Ongoing Projects, Topics to be covered, Prerequisites, etc. by our Internship Supervisor Dr. Muneendra Ojha.<br>
4 June 24 : Linear regression, logistic regression (loss functions).<br>
5 June 24 : Artificial Nueral Networks.<br>
6 June 24 : Learnable parameters in neural nets: weights and biases.<br>
7 June 24 : How learning in neural nets happen : forward and backpropagation.<br>
10 June 24 : Recurrent Neural Networks and decided to work on the project based on Swarm Robotics and Deep Reinforcement Learning.<br>
11 June 24 : Given with two research papers on Deep Reinforcement Learning (Attached in this repository) and told to learn the basics of deep learning (started doing that).<br>
12 June 24: Continued with learning basics of Deep learning and Supervised Learning.<br>
13 June 24 : Started with studying research paper on " Decentralized Multi-Agent Pursuit Using Deep Reinforcement Learning " .<br>
14 June 24 : Analysis of Second Research Paper on  " Game of Drones: Multi-UAV Pursuit-Evasion Game With Online Motion Planning by Deep Reinforcement Learning ".<br>
17 June 14 : Learned about "Pygame" to create simulation environment for our project.<br>
18 June 24 : Made Presentations for our research paper as a testimony (Attached in the repo.)<br>
19 June 24 : Were given a website link to analyse, study and implement a sample simulation environment using pygame. Link : https://medium.com/agents-and-robots/a-multi-agent-system-in-python-74701f256c3a<br>
20 June 24 : Were given the task to analyse the action space and state space of the research paper and told to make chamges in the simulation created before as per the research papers.<br>
21 June 24 : Sterted learning and understanding about the simulation environment and started doing changes in it as per the paper.<br>
24 June 24 : Analysed the Action space , state space and rewarding in paper 1 and created a simulation for the same.<br>
25 June 24 : Started studying and analysing the Implementation of DRL in paper 1 and trying to avoid collisions between evadors as well between pursuers and evadors.<br>
26 June 24 : Created a simulation that avoided collisons within evadors as well as collisions with agents and evadors using direct approach.<br>
27 June 24 : Tried to implement the DRL approaches like DDPG and PPO to avoid collisons.<br>
28 June 24 : Continued working on implementation of DRL approaches. <br>
1 July 24 : Studied about the implementation of DRL approaches using various python libraries and modules.<br>
2 July 24 : Started creating the simulation for evasive strategy by defining the model features of PPO on my own.<br>
3 July 24 : Continued working on the same.<br>
4 July 24 : Showed the basic layout made so far to the instructor and took the feedback and recommendations.<br>
5 July 24 : Studied more about how models are implemeted without pre-defined libraries and how models are saved.<br>
8 July 24 : Made further changes in my implementation so that evadors avoid pursuers and the inter-evador collisons.<br>
9 July 24 : Continued working on the same.<br>
10 July 24 : It had many errors , so took the help from the instructor.<br>
11 July 24 : Instructor directed to change the approach by using some python library to implement model since it is preffered. So, started studying about python libraries for implementation of my desired model i.e. PPO.<br>
12 July 24 : After collecting and researching, found a renowned python library named stable baselines 3 for my model's implementation. Started reading about it's implementation in the code.<br>
15 July 24 : Created a simulation with my required features using stable baselines and taking the environment from ChatGPT's Gym.<br>
16 July 24 : Still working on training and tuning my agents' movements.<br>
